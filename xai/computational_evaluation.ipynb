{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save models and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(filename, model, X_test, y_test, features):    \n",
    "    try:\n",
    "        import os\n",
    "        import re\n",
    "        import pickle\n",
    "        import socket\n",
    "        import datetime\n",
    "\n",
    "        hostname = re.sub(\"\\..*\", \"\", socket.gethostname ())\n",
    "        hostname = hostname.lower()\n",
    "        print(\"Hostname: \" + hostname)\n",
    "\n",
    "        now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "        filepath = \"C:\\\\PrecareML\\\\XAI\\\\Modelle\\\\\" + filename + \"_\" + hostname + \"_\" + now + \".hdfs\"\n",
    "        \n",
    "        model_dict = {\"model\": model, \"X_test\": X_test, \"y_test\": y_test, \"features\": features}\n",
    "        \n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(model_dict, f)\n",
    "        \n",
    "        return \"Object has been saved!\"\n",
    "    except:\n",
    "        print(\"An exception occurred in function save_model\")    \n",
    "        return \"Object hasn't been saved!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_explainer(filename, expl):    \n",
    "    import socket\n",
    "    import pickle\n",
    "    import dill\n",
    "    #import pandas as pd\n",
    "    #import numpy as np\n",
    "    import os\n",
    "    import re\n",
    "\n",
    "    hostname = re.sub(\"\\..*\", \"\", socket.gethostname ())\n",
    "    hostname = hostname.lower()\n",
    "    print(\"Hostname: \" + hostname)\n",
    "\n",
    "    now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    filepath = \"C:\\\\PrecareML\\\\XAI\\\\Modelle\\\\\" + filename + \"_\" + hostname + \"_\" + now + \".hdfs\"\n",
    "        \n",
    "        \n",
    "    model_dict = {\"model\": expl}\n",
    "        \n",
    "    with open(filepath, 'wb') as f:\n",
    "       # pickle.dump(model_dict, f)\n",
    "        dill.dump(model_dict, f)\n",
    "        \n",
    "    return \"Object has been saved!\"\n",
    "    \n",
    "####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(filename, size, data):    \n",
    "    import socket\n",
    "    import pickle\n",
    "    #import dill\n",
    "    #import pandas as pd\n",
    "    #import numpy as np\n",
    "    import os\n",
    "    import re\n",
    "\n",
    "    hostname = re.sub(\"\\..*\", \"\", socket.gethostname ())\n",
    "    hostname = hostname.lower()\n",
    "    print(\"Hostname: \" + hostname)\n",
    "\n",
    "    now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    filepath = \"C:\\\\PrecareML\\\\XAI\\\\Daten\\\\\" + filename+str(size) + \"_\" + hostname + \"_\" + now + \".hdfs\"\n",
    "        \n",
    "        \n",
    "    model_dict = {\"data\": data}\n",
    "        \n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(model_dict, f)\n",
    "        #dill.dump(model_dict, f)\n",
    "        \n",
    "    return \"Data has been saved!\"\n",
    "    \n",
    "####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "trainingData = pd.read_csv ('trainingData.csv')\n",
    "trainingData.rename(columns = {'.outcome' : 'LABEL'}, inplace = True)\n",
    "\n",
    "trainingData['LABEL'] = trainingData['LABEL'].str.replace('NO','0')\n",
    "trainingData['LABEL'] = trainingData['LABEL'].str.replace('Y','1')\n",
    "trainingData['LABEL'] = trainingData['LABEL'].astype('category')\n",
    "\n",
    "trainingData.drop(columns=trainingData.columns[0], \n",
    "        axis=1, \n",
    "        inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(trainingData.drop('DELIR', 1).values, trainingData['DELIR'].to_numpy(), test_size=0.3, random_state=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Prediction Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import multiprocessing\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
    "\n",
    "n_defined_jobs = multiprocessing.cpu_count() - 2\n",
    "param_grid = {\n",
    "    'n_estimators': [64, 256, 1024],\n",
    "    'max_depth': [8, 16, None],\n",
    "    'criterion': [\"gini\"]\n",
    "             }\n",
    "\n",
    "def trainRFModel():\n",
    "    clf = RandomForestClassifier()\n",
    "\n",
    "    prediction_model = GridSearchCV(clf, param_grid, cv=5, n_jobs = n_defined_jobs)\n",
    "\n",
    "    prediction_model.fit(X_train, y_train)\n",
    "\n",
    "    save_model(\"prediction_model\", prediction_model, X_test, y_test, features)\n",
    "\n",
    "    return prediction_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = prediction_model.predict_proba(X_test)\n",
    "y_pred_classes =  prediction_model.predict(X_test)\n",
    "model_auc = roc_auc_score(y_test, y_pred[:, 1])\n",
    "print(model_auc)\n",
    "classificationReport = classification_report(y_test, y_pred_classes)\n",
    "print(classificationReport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Explanation Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 200\n",
    "isFullRun = False\n",
    "data_to_be_explained = trainingData.drop('LABEL', 1).values\n",
    "\n",
    "if isFullRun:\n",
    "    data_size = len(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_tabular\n",
    "\n",
    "lime_explainer = lime_tabular.LimeTabularExplainer(data_to_be_explained\n",
    "                                               ,feature_names = features \n",
    "                                               ,class_names=['No','Yes']\n",
    "                                               ,mode='classification'\n",
    "                                               ,feature_selection= 'lasso_path' \n",
    "                                               ,discretize_continuous=True\n",
    "                                               ,discretizer='quartile'\n",
    "                                             )\n",
    "\n",
    "predict_fn = lambda x: prediction_model.predict_proba(x).astype(float)\n",
    "\n",
    "fi_df = pd.DataFrame(0.0, index=np.arange(data_size),columns=features)\n",
    "\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "print(now)\n",
    "        \n",
    "for p in range(0,data_size):\n",
    "    lime_explanation = lime_explainer.explain_instance(data_to_be_explained[p]\n",
    "                                          ,predict_fn\n",
    "                                          ,num_features=10\n",
    "                                          #,top_labels=1\n",
    "                                          #,distance_metric='cosine'\n",
    "                                          #,distance_metric='manhattan'\n",
    "                                        )\n",
    "   \n",
    "    fi_lime = lime_explanation.as_list()\n",
    "    #print(fi_lime)\n",
    "    \n",
    "    fi_lime = pd.DataFrame(fi_lime, columns=['feature','importance'])\n",
    "    for i in range(0,len(fi_lime)): \n",
    "        splitted = fi_lime.loc[i, 'feature'].split(' ')\n",
    "        #splitted = fi_lime['feature'][i].split(' ')\n",
    "        if len(splitted)==3:\n",
    "            fi_lime.loc[i, 'feature'] = splitted[0]\n",
    "            #fi_lime['feature'][i] = splitted[0]\n",
    "        else:\n",
    "            fi_lime.loc[i, 'feature'] = splitted[2]\n",
    "            #fi_lime['feature'][i] = splitted[2]\n",
    "    \n",
    "    for i in range(0,len(fi_lime)):\n",
    "        fi_df.loc[p,fi_lime['feature'][i]]= fi_lime.loc[i,'importance']\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "print(now)\n",
    "save_data(\"LIME\",data_size, fi_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap \n",
    "shap.initjs()\n",
    "    \n",
    "# Define a tree explainer for the built model\n",
    "explainer_shap = shap.TreeExplainer(prediction_model.best_estimator_)\n",
    "\n",
    "fi_shap_df = pd.DataFrame(0.0, index=np.arange(data_size),columns=features)\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "print(now)\n",
    "\n",
    "for p in range(0,data_size):\n",
    "    # obtain shap values for the chosen row of the test data\n",
    "    shap_values = explainer_shap.shap_values(data_to_be_explained[p])\n",
    "   \n",
    "    #fi_shap = pd.DataFrame(shap_values, columns=['importance'])\n",
    "    for f in range(0,len(features)): \n",
    "        fi_shap_df.loc[p,features[f]]= shap_values[0][f]\n",
    "        \n",
    "now = datetime.datetime.now()\n",
    "print(now)        \n",
    "save_data(\"SHAP\",data_size, fi_shap_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Explanation Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from math import sqrt\n",
    "\n",
    "def roc_auc_ci(y_true, y_score):\n",
    "    positive=1\n",
    "    AUC = roc_auc_score(y_true, y_score)\n",
    "    N1 = sum(y_true == positive)\n",
    "    N2 = sum(y_true != positive)\n",
    "    Q1 = AUC / (2 - AUC)\n",
    "    Q2 = 2*AUC**2 / (1 + AUC)\n",
    "    SE_AUC = sqrt((AUC*(1 - AUC) + (N1 - 1)*(Q1 - AUC**2) + (N2 - 1)*(Q2 - AUC**2)) / (N1*N2))\n",
    "    lower = AUC - 1.96*SE_AUC\n",
    "    upper = AUC + 1.96*SE_AUC\n",
    "    if lower < 0:\n",
    "        lower = 0\n",
    "    if upper > 1:\n",
    "        upper = 1\n",
    "    return (str(AUC)+ \",\" +str(lower) +\"-\"+str(upper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeLong CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from\n",
    "# https://github.com/yandexdataschool/roc_comparison\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "def compute_midrank(x):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float64)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = 0.5*(i + j - 1)\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float64)\n",
    "    # Note(kazeevn) +1 is due to Python using 0-based indexing\n",
    "    # instead of 1-based in the AUC formula in the paper\n",
    "    T2[J] = T + 1\n",
    "    return T2\n",
    "\n",
    "\n",
    "def compute_midrank_weight(x, sample_weight):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    cumulative_weight = np.cumsum(sample_weight[J])\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float64)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = cumulative_weight[i:j].mean()\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float64)\n",
    "    T2[J] = T\n",
    "    return T2\n",
    "\n",
    "\n",
    "def fastDeLong(predictions_sorted_transposed, label_1_count, sample_weight):\n",
    "    if sample_weight is None:\n",
    "        return fastDeLong_no_weights(predictions_sorted_transposed, label_1_count)\n",
    "    else:\n",
    "        return fastDeLong_weights(predictions_sorted_transposed, label_1_count, sample_weight)\n",
    "\n",
    "\n",
    "def fastDeLong_weights(predictions_sorted_transposed, label_1_count, sample_weight):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=np.float64)\n",
    "    ty = np.empty([k, n], dtype=np.float64)\n",
    "    tz = np.empty([k, m + n], dtype=np.float64)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank_weight(positive_examples[r, :], sample_weight[:m])\n",
    "        ty[r, :] = compute_midrank_weight(negative_examples[r, :], sample_weight[m:])\n",
    "        tz[r, :] = compute_midrank_weight(predictions_sorted_transposed[r, :], sample_weight)\n",
    "    total_positive_weights = sample_weight[:m].sum()\n",
    "    total_negative_weights = sample_weight[m:].sum()\n",
    "    pair_weights = np.dot(sample_weight[:m, np.newaxis], sample_weight[np.newaxis, m:])\n",
    "    total_pair_weights = pair_weights.sum()\n",
    "    aucs = (sample_weight[:m]*(tz[:, :m] - tx)).sum(axis=1) / total_pair_weights\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / total_negative_weights\n",
    "    v10 = 1. - (tz[:, m:] - ty[:, :]) / total_positive_weights\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def fastDeLong_no_weights(predictions_sorted_transposed, label_1_count):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating\n",
    "              Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=np.float64)\n",
    "    ty = np.empty([k, n], dtype=np.float64)\n",
    "    tz = np.empty([k, m + n], dtype=np.float64)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank(positive_examples[r, :])\n",
    "        ty[r, :] = compute_midrank(negative_examples[r, :])\n",
    "        tz[r, :] = compute_midrank(predictions_sorted_transposed[r, :])\n",
    "    aucs = tz[:, :m].sum(axis=1) / m / n - float(m + 1.0) / 2.0 / n\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / n\n",
    "    v10 = 1.0 - (tz[:, m:] - ty[:, :]) / m\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def calc_pvalue(aucs, sigma):\n",
    "    \"\"\"Computes log(10) of p-values.\n",
    "    Args:\n",
    "       aucs: 1D array of AUCs\n",
    "       sigma: AUC DeLong covariances\n",
    "    Returns:\n",
    "       log10(pvalue)\n",
    "    \"\"\"\n",
    "    l = np.array([[1, -1]])\n",
    "    z = np.abs(np.diff(aucs)) / np.sqrt(np.dot(np.dot(l, sigma), l.T))\n",
    "    return np.log10(2) + scipy.stats.norm.logsf(z, loc=0, scale=1) / np.log(10)\n",
    "\n",
    "\n",
    "def compute_ground_truth_statistics(ground_truth, sample_weight):\n",
    "    assert np.array_equal(np.unique(ground_truth), [0, 1])\n",
    "    order = (-ground_truth).argsort()\n",
    "    label_1_count = int(ground_truth.sum())\n",
    "    if sample_weight is None:\n",
    "        ordered_sample_weight = None\n",
    "    else:\n",
    "        ordered_sample_weight = sample_weight[order]\n",
    "\n",
    "    return order, label_1_count, ordered_sample_weight\n",
    "\n",
    "\n",
    "def delong_roc_variance(ground_truth, predictions, sample_weight=None):\n",
    "    \"\"\"\n",
    "    Computes ROC AUC variance for a single set of predictions\n",
    "    Args:\n",
    "       ground_truth: np.array of 0 and 1\n",
    "       predictions: np.array of floats of the probability of being class 1\n",
    "    \"\"\"\n",
    "    order, label_1_count, ordered_sample_weight = compute_ground_truth_statistics(\n",
    "        ground_truth, sample_weight)\n",
    "    predictions_sorted_transposed = predictions[np.newaxis, order]\n",
    "    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count, ordered_sample_weight)\n",
    "    assert len(aucs) == 1, \"There is a bug in the code, please forward this to the developers\"\n",
    "    return aucs[0], delongcov\n",
    "\n",
    "\n",
    "def roc_auc_deLong_ci(y_true,y_pred):\n",
    "    alpha = .95\n",
    "    #y_pred = np.array([0.21, 0.32, 0.63, 0.35, 0.92, 0.79, 0.82, 0.99, 0.04])\n",
    "    #y_true = np.array([0,    1,    0,    0,    1,    1,    0,    1,    0   ])\n",
    "\n",
    "    auc, auc_cov = delong_roc_variance(\n",
    "        y_true,\n",
    "        y_pred)\n",
    "\n",
    "    auc_std = np.sqrt(auc_cov)\n",
    "    lower_upper_q = np.abs(np.array([0, 1]) - (1 - alpha) / 2)\n",
    "\n",
    "    ci = stats.norm.ppf(\n",
    "        lower_upper_q,\n",
    "        loc=auc,\n",
    "        scale=auc_std)\n",
    "\n",
    "    ci[ci > 1] = 1\n",
    "\n",
    "    print('AUC:', auc)\n",
    "    print('AUC COV:', auc_cov)\n",
    "    print('95% AUC CI:', ci)\n",
    "    return (str(round(auc, 2)) + \" (\" + str(round(ci[0], 2))+ \"-\"+ str(round(ci[1], 2))+ \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compared with expected prognoses\n",
    "auroc_results_df = pd.DataFrame('0.0', index=['PM','EM_LIME','EM_SHAP'],columns=['Values', 'FI', 'FI*Values', 'FI*N_Values'])\n",
    "\n",
    "# compared with predicted prognoses risks\n",
    "rmse_results_df = pd.DataFrame('0.0', index=['EM_LIME','EM_SHAP'],columns=['FI', 'FI*Values', 'FI*N_Values'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Dataset 0: (val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Same dataset is predicted with RF\n",
    "X_train_rf3, X_test_rf3, y_train_rf3, y_test_rf3 = train_test_split(X_test[0:data_size], y_test[0:data_size], test_size=0.3, random_state=16)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "eval_model_rf3 = GridSearchCV(clf, param_grid, cv=5, n_jobs = n_defined_jobs) \n",
    "eval_model_rf3.fit(X_train_rf3, y_train_rf3)\n",
    "y_pred_rf3 = eval_model_rf3.predict_proba(X_test_rf3)\n",
    "y_pred_classes_rf3 =  eval_model_rf3.predict(X_test_rf3)\n",
    "\n",
    "# compare with Test Label: y_test_eval1c\n",
    "model_auc_rf3 = roc_auc_score(y_test_rf3, y_pred_rf3[:, 1])\n",
    "classificationReport_rf3 = classification_report(y_test_rf3, y_pred_classes_rf3)\n",
    "print(\"AUROC:\")\n",
    "print(model_auc_rf3)\n",
    "print(classificationReport_rf3)\n",
    "\n",
    "auroc_results_df.loc['PM','Values'] = roc_auc_deLong_ci(np.array(y_test_rf3).astype(int), np.array(y_pred_rf3[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset 1: (fi_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset1.Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIME\n",
    "X_train_eval1, X_test_eval1, y_train_eval1, y_test_eval1 = train_test_split(fi_df.values, y_pred[0:data_size], test_size=0.3, random_state=16)\n",
    "\n",
    "# SHAP\n",
    "X_train_eval1s, X_test_eval1s, y_train_eval1s, y_test_eval1s = train_test_split(fi_shap_df.values, y_pred[0:data_size], test_size=0.3, random_state=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset1.Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIME\n",
    "X_train_eval1c, X_test_eval1c, y_train_eval1c, y_test_eval1c = train_test_split(fi_df.values, y_test[0:data_size], test_size=0.3, random_state=16)\n",
    "\n",
    "# SHAP\n",
    "X_train_eval1sc, X_test_eval1sc, y_train_eval1sc, y_test_eval1sc = train_test_split(fi_shap_df.values, y_test[0:data_size], test_size=0.3, random_state=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RF with LIME data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results of Evaluation Model.1 for LIME\")\n",
    "\n",
    "#####################\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "eval_model1 = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "eval_model1.fit(X_train_eval1, y_train_eval1)\n",
    "save_model(\"eval_model1\", eval_model1, X_test_eval1, y_test_eval1, features)\n",
    "y_pred1 = eval_model1.predict(X_test_eval1)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "# compare with Predictions: y_test_eval1\n",
    "MSE1 = mean_squared_error(y_test_eval1[:, 1], y_pred1[:, 1])\n",
    "RMSE1 = math.sqrt(MSE1)\n",
    "print(\"Root Mean Square Error:\")\n",
    "print(RMSE1)\n",
    "rmse_results_df.loc['EM_LIME','FI'] = RMSE1 \n",
    "####################\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "eval_model1c = GridSearchCV(clf, param_grid, cv=5, n_jobs = n_defined_jobs) \n",
    "eval_model1c.fit(X_train_eval1c, y_train_eval1c)\n",
    "y_pred1c = eval_model1c.predict_proba(X_test_eval1c)\n",
    "y_pred_classes1c =  eval_model1c.predict(X_test_eval1c)\n",
    "\n",
    "# compare with Test Label: y_test_eval1c\n",
    "model_auc1 = roc_auc_score(y_test_eval1c, y_pred1c[:, 1])\n",
    "classificationReport1 = classification_report(y_test_eval1c, y_pred_classes1c)\n",
    "print(\"AUROC:\")\n",
    "print(model_auc1)\n",
    "print(classificationReport1)\n",
    "auroc_results_df.loc['EM_LIME','FI'] = roc_auc_deLong_ci(np.array(y_test_eval1c).astype(int), np.array(y_pred1c[:, 1]))\n",
    "#####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RF with SHAP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = RandomForestClassifier()\n",
    "#eval_model1s = GridSearchCV(clf, param_grid, cv=5, n_jobs = n_defined_jobs) \n",
    "eval_model1s = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "eval_model1s.fit(X_train_eval1s, y_train_eval1s)\n",
    "save_model(\"eval_model1s\", eval_model1s, X_test_eval1s, y_test_eval1s, features)\n",
    "\n",
    "y_pred1s = eval_model1s.predict(X_test_eval1s)\n",
    "#y_pred_classes1s =  eval_model1s.predict(X_test_eval1s)\n",
    "#model_auc1s = roc_auc_score(y_test_eval1s, y_pred1s[:, 1])\n",
    "#classificationReport1s = classification_report(y_test_eval1s, y_pred_classes1s)\n",
    "\n",
    "print(\"Results of Evaluation Model.1 for SHAP\")\n",
    "#print(\"AUROC:\")\n",
    "#print(model_auc1s)\n",
    "#print(classificationReport1s)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "MSE1s = mean_squared_error(y_test_eval1s[:, 1], y_pred1s[:, 1])\n",
    "RMSE1s = math.sqrt(MSE1s)\n",
    "print(\"Root Mean Square Error:\")\n",
    "print(RMSE1s)\n",
    "rmse_results_df.loc['EM_SHAP','FI'] = RMSE1s \n",
    "#####################\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "eval_model1sc = GridSearchCV(clf, param_grid, cv=5, n_jobs = n_defined_jobs) \n",
    "eval_model1sc.fit(X_train_eval1sc, y_train_eval1sc)\n",
    "y_pred1sc = eval_model1sc.predict_proba(X_test_eval1sc)\n",
    "y_pred_classes1sc =  eval_model1sc.predict(X_test_eval1sc)\n",
    "\n",
    "# compare with Test Label: y_test_eval1c\n",
    "model_auc1sc = roc_auc_score(y_test_eval1sc, y_pred1sc[:, 1])\n",
    "classificationReport1sc = classification_report(y_test_eval1sc, y_pred_classes1sc)\n",
    "print(\"AUROC:\")\n",
    "print(model_auc1sc)\n",
    "print(classificationReport1sc)\n",
    "auroc_results_df.loc['EM_SHAP','FI'] = roc_auc_deLong_ci(np.array(y_test_eval1sc).astype(int), np.array(y_pred1sc[:, 1]))\n",
    "#####################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset.2 (fi_val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_val_df = pd.DataFrame(0.0, index=np.arange(data_size),columns=features) # TODO: change size\n",
    "val_df = pd.DataFrame(X_test[0:data_size],columns=features)\n",
    "\n",
    "for f in features:\n",
    "    fi_val_df[f] = fi_df[f] * val_df[f]\n",
    "    \n",
    "X_train_eval2, X_test_eval2, y_train_eval2, y_test_eval2 = train_test_split(fi_val_df.values, y_pred[0:data_size], test_size=0.3, random_state=16)\n",
    "X_train_eval2c, X_test_eval2c, y_train_eval2c, y_test_eval2c = train_test_split(fi_val_df.values, y_test[0:data_size], test_size=0.3, random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_shap_val_df = pd.DataFrame(0.0, index=np.arange(data_size),columns=features) # TODO: change size\n",
    "\n",
    "for f in features:\n",
    "    fi_shap_val_df[f] = fi_shap_df[f] * val_df[f]\n",
    "    \n",
    "X_train_eval2s, X_test_eval2s, y_train_eval2s, y_test_eval2s = train_test_split(fi_shap_val_df.values, y_pred[0:data_size], test_size=0.3, random_state=16)\n",
    "X_train_eval2sc, X_test_eval2sc, y_train_eval2sc, y_test_eval2sc = train_test_split(fi_shap_val_df.values, y_test[0:data_size], test_size=0.3, random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = RandomForestClassifier()\n",
    "#eval_model2 = GridSearchCV(clf, param_grid, cv=5, n_jobs = n_defined_jobs)\n",
    "eval_model2 = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "\n",
    "eval_model2.fit(X_train_eval2, y_train_eval2)\n",
    "save_model(\"eval_model2\", eval_model2, X_test_eval2, y_test_eval2, features)\n",
    "\n",
    "y_pred2 = eval_model2.predict(X_test_eval2)\n",
    "#y_pred_classes2 =  eval_model2.predict(X_test_eval2)\n",
    "#model_auc2 = roc_auc_score(y_test_eval2, y_pred2[:, 1])\n",
    "#classificationReport2 = classification_report(y_test_eval2, y_pred_classes2)\n",
    "\n",
    "print(\"Results of Evaluation Model.2 for LIME:\")\n",
    "#print(\"AUROC:\")\n",
    "#print(model_auc2)\n",
    "#print(classificationReport2)\n",
    "\n",
    "MSE2 = mean_squared_error(y_test_eval2[:, 1], y_pred2[:, 1])\n",
    "RMSE2 = math.sqrt(MSE2)\n",
    "print(\"Root Mean Square Error:\")\n",
    "print(RMSE2)\n",
    "rmse_results_df.loc['EM_LIME','FI*Values'] = RMSE2 \n",
    "#################################################\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "eval_model2c = GridSearchCV(clf, param_grid, cv=5, n_jobs = n_defined_jobs) \n",
    "eval_model2c.fit(X_train_eval2c, y_train_eval2c)\n",
    "y_pred2c = eval_model2c.predict_proba(X_test_eval2c)\n",
    "y_pred_classes2c =  eval_model2c.predict(X_test_eval2c)\n",
    "\n",
    "# compare with Test Label: y_test_eval1c\n",
    "model_auc2c = roc_auc_score(y_test_eval2c, y_pred2c[:, 1])\n",
    "classificationReport2c = classification_report(y_test_eval2c, y_pred_classes2c)\n",
    "print(\"AUROC:\")\n",
    "print(model_auc2c)\n",
    "print(classificationReport2c)\n",
    "auroc_results_df.loc['EM_LIME','FI*Values'] = roc_auc_deLong_ci(np.array(y_test_eval2c).astype(int), np.array(y_pred2c[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = RandomForestClassifier()\n",
    "#eval_model2s = GridSearchCV(clf, param_grid, cv=5, n_jobs = n_defined_jobs)\n",
    "eval_model2s = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "eval_model2s.fit(X_train_eval2s, y_train_eval2s)\n",
    "save_model(\"eval_model2s\", eval_model2s, X_test_eval2s, y_test_eval2s, features)\n",
    "\n",
    "y_pred2s = eval_model2s.predict(X_test_eval2s)\n",
    "#y_pred_classes2s =  eval_model2s.predict(X_test_eval2s)\n",
    "#model_auc2s = roc_auc_score(y_test_eval2s, y_pred2s[:, 1])\n",
    "#classificationReport2s = classification_report(y_test_eval2s, y_pred_classes2s)\n",
    "\n",
    "print(\"Results of Evaluation Model.2 for SHAP:\")\n",
    "#print(\"AUROC:\")\n",
    "#print(model_auc2s)\n",
    "#print(classificationReport2s)\n",
    "\n",
    "MSE2s = mean_squared_error(y_test_eval2s[:, 1], y_pred2s[:, 1])\n",
    "RMSE2s = math.sqrt(MSE2s)\n",
    "print(\"Root Mean Square Error:\")\n",
    "print(RMSE2s)\n",
    "rmse_results_df.loc['EM_SHAP','FI*Values'] = RMSE2s \n",
    "#################################################\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "eval_model2sc = GridSearchCV(clf, param_grid, cv=5, n_jobs = n_defined_jobs) \n",
    "eval_model2sc.fit(X_train_eval2sc, y_train_eval2sc)\n",
    "y_pred2sc = eval_model2sc.predict_proba(X_test_eval2sc)\n",
    "y_pred_classes2sc =  eval_model2sc.predict(X_test_eval2sc)\n",
    "\n",
    "# compare with Test Label: y_test_eval1c\n",
    "model_auc2sc = roc_auc_score(y_test_eval2sc, y_pred2sc[:, 1])\n",
    "classificationReport2sc = classification_report(y_test_eval2sc, y_pred_classes2sc)\n",
    "print(\"AUROC:\")\n",
    "print(model_auc2sc)\n",
    "print(classificationReport2sc)\n",
    "auroc_results_df.loc['EM_SHAP','FI*Values'] = roc_auc_deLong_ci(np.array(y_test_eval2sc).astype(int), np.array(y_pred2sc[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset.3 (fi_norm_val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_val_df = val_df.copy()\n",
    "\n",
    "# normalize all columns using min-max normalization\n",
    "for c in features:\n",
    "    norm_val_df[c] = (norm_val_df[c] - norm_val_df[c].min()) / (norm_val_df[c].max() - norm_val_df[c].min())    \n",
    "    \n",
    "# if min and max both are 0, normalized value makes NaN\n",
    "norm_val_df = norm_val_df.fillna(0)\n",
    "\n",
    "fi_norm_val_df = pd.DataFrame(0.0, index=np.arange(data_size),columns=features)\n",
    "\n",
    "for f in features:\n",
    "    fi_norm_val_df[f] = fi_df[f] * norm_val_df[f]\n",
    "    \n",
    "X_train_eval3, X_test_eval3, y_train_eval3, y_test_eval3 = train_test_split(fi_norm_val_df.values, y_pred[0:data_size], test_size=0.3, random_state=16)\n",
    "X_train_eval3c, X_test_eval3c, y_train_eval3c, y_test_eval3c = train_test_split(fi_norm_val_df.values, y_test[0:data_size], test_size=0.3, random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_shap_norm_val_df = pd.DataFrame(0.0, index=np.arange(data_size),columns=features)\n",
    "\n",
    "for f in features:\n",
    "    fi_shap_norm_val_df[f] = fi_shap_df[f] * norm_val_df[f]\n",
    "    \n",
    "X_train_eval3s, X_test_eval3s, y_train_eval3s, y_test_eval3s = train_test_split(fi_shap_norm_val_df.values, y_pred[0:data_size], test_size=0.3, random_state=16)\n",
    "X_train_eval3sc, X_test_eval3sc, y_train_eval3sc, y_test_eval3sc = train_test_split(fi_shap_norm_val_df.values, y_test[0:data_size], test_size=0.3, random_state=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = RandomForestClassifier()\n",
    "#eval_model3 = GridSearchCV(clf, param_grid, cv=5, n_jobs = n_defined_jobs)\n",
    "\n",
    "eval_model3 = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "eval_model3.fit(X_train_eval3, y_train_eval3)\n",
    "save_model(\"eval_model3\", eval_model3, X_test_eval3, y_test_eval3, features)\n",
    "\n",
    "y_pred3 = eval_model3.predict(X_test_eval3)\n",
    "#y_pred_classes3 =  eval_model3.predict(X_test_eval3)\n",
    "#model_auc3 = roc_auc_score(y_test_eval3, y_pred3[:, 1])\n",
    "#classificationReport3 = classification_report(y_test_eval3, y_pred_classes3)\n",
    "\n",
    "print(\"Results of Evaluation Model.3 for LIME:\")\n",
    "#print(\"AUROC:\")\n",
    "#print(model_auc3)\n",
    "#print(classificationReport3)\n",
    "\n",
    "MSE3 = mean_squared_error(y_test_eval3[:, 1], y_pred3[:, 1])\n",
    "RMSE3 = math.sqrt(MSE3)\n",
    "print(\"Root Mean Square Error:\")\n",
    "print(RMSE3)\n",
    "rmse_results_df.loc['EM_LIME','FI*N_Values'] = RMSE3 \n",
    "#################################################\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "eval_model3c = GridSearchCV(clf, param_grid, cv=5, n_jobs = n_defined_jobs) \n",
    "eval_model3c.fit(X_train_eval3c, y_train_eval3c)\n",
    "y_pred3c = eval_model3c.predict_proba(X_test_eval3c)\n",
    "y_pred_classes3c =  eval_model3c.predict(X_test_eval3c)\n",
    "\n",
    "# compare with Test Label: y_test_eval1c\n",
    "model_auc3c = roc_auc_score(y_test_eval3c, y_pred3c[:, 1])\n",
    "classificationReport3c = classification_report(y_test_eval3c, y_pred_classes3c)\n",
    "print(\"AUROC:\")\n",
    "print(model_auc3c)\n",
    "print(classificationReport3c)\n",
    "auroc_results_df.loc['EM_LIME','FI*N_Values'] = roc_auc_deLong_ci(np.array(y_test_eval3c).astype(int), np.array(y_pred3c[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "fpr, tpr, _ = metrics.roc_curve(np.array(y_test_eval3c).astype(int),  y_pred3c[:, 1])\n",
    "\n",
    "fpr_pm, tpr_pm, _ = metrics.roc_curve(np.array(y_test_eval1c).astype(int),  y_pred1c[:, 1])\n",
    "\n",
    "#create ROC curve\n",
    "plt.plot(fpr,tpr,label=\"EM: AUROC (95% CI)= \"+auroc_results_df.loc['EM_LIME','FI*N_Values'])\n",
    "plt.plot(fpr_pm,tpr_pm,label=\"PM: AUROC (95% CI)= \"+auroc_results_df.loc['PM','Values'])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = RandomForestClassifier()\n",
    "#eval_model3s = GridSearchCV(clf, param_grid, cv=5, n_jobs = n_defined_jobs)\n",
    "\n",
    "eval_model3s = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "eval_model3s.fit(X_train_eval3s, y_train_eval3s)\n",
    "save_model(\"eval_model3s\", eval_model3s, X_test_eval3s, y_test_eval3s, features)\n",
    "\n",
    "y_pred3s = eval_model3s.predict(X_test_eval3s)\n",
    "#y_pred_classes3s =  eval_model3s.predict(X_test_eval3s)\n",
    "#model_auc3s = roc_auc_score(y_test_eval3s, y_pred3s[:, 1])\n",
    "#classificationReport3s = classification_report(y_test_eval3s, y_pred_classes3)\n",
    "\n",
    "print(\"Results of Evaluation Model.3 for SHAP:\")\n",
    "#print(\"AUROC:\")\n",
    "#print(model_auc3s)\n",
    "#print(classificationReport3s)\n",
    "\n",
    "MSE3s = mean_squared_error(y_test_eval3[:, 1], y_pred3[:, 1])\n",
    "RMSE3s = math.sqrt(MSE3s)\n",
    "print(\"Root Mean Square Error:\")\n",
    "print(RMSE3s)\n",
    "rmse_results_df.loc['EM_SHAP','FI*N_Values'] = RMSE3s \n",
    "#################################################\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "eval_model3sc = GridSearchCV(clf, param_grid, cv=5, n_jobs = n_defined_jobs) \n",
    "eval_model3sc.fit(X_train_eval3sc, y_train_eval3sc)\n",
    "y_pred3sc = eval_model3sc.predict_proba(X_test_eval3sc)\n",
    "y_pred_classes3sc =  eval_model3sc.predict(X_test_eval3sc)\n",
    "\n",
    "# compare with Test Label: y_test_eval1c\n",
    "model_auc3sc = roc_auc_score(y_test_eval3sc, y_pred3sc[:, 1])\n",
    "classificationReport3sc = classification_report(y_test_eval3sc, y_pred_classes3sc)\n",
    "print(\"AUROC:\")\n",
    "print(model_auc3sc)\n",
    "print(classificationReport3sc)\n",
    "auroc_results_df.loc['EM_SHAP','FI*N_Values'] = roc_auc_deLong_ci(np.array(y_test_eval3sc).astype(int), np.array(y_pred3sc[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOXPLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# BEFORE\n",
    "df_pred = pd.DataFrame(y_pred_rf3[:, 1], columns = ['Predicted'])\n",
    "df_pred['Observed'] = y_test_rf3\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "ax = sns.boxplot(x='Observed',y='Predicted',data=df_pred)\n",
    "ax = sns.stripplot(x=\"Observed\", y=\"Predicted\",data=df_pred)\n",
    "plt.title(\"Predictions before XAI\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# AFTER.1: \n",
    "df_pred = pd.DataFrame(y_pred1c[:, 1], columns = ['Predicted'])\n",
    "df_pred['Observed'] = y_test_eval1c\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "ax = sns.boxplot(x='Observed',y='Predicted',data=df_pred)\n",
    "ax = sns.stripplot(x=\"Observed\", y=\"Predicted\",data=df_pred)\n",
    "plt.title(\"Predictions after XAI (LIME, FI)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# AFTER.2\n",
    "df_pred = pd.DataFrame(y_pred2c[:, 1], columns = ['Predicted'])\n",
    "df_pred['Observed'] = y_test_eval2c\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "ax= sns.boxplot(x='Observed',y='Predicted',data=df_pred)\n",
    "ax = sns.stripplot(x=\"Observed\", y=\"Predicted\",data=df_pred)\n",
    "plt.title(\"Predictions after XAI (LIME, FI*Values)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# AFTER.3\n",
    "df_pred = pd.DataFrame(y_pred3c[:, 1], columns = ['Predicted'])\n",
    "df_pred['Observed'] = y_test_eval3c\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "ax= sns.boxplot(x='Observed',y='Predicted',data=df_pred)\n",
    "ax = sns.stripplot(x=\"Observed\", y=\"Predicted\",data=df_pred)\n",
    "plt.title(\"Predictions after XAI (LIME, FI*Norm_Values)\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# AFTER.4\n",
    "df_pred = pd.DataFrame(y_pred1sc[:, 1], columns = ['Predicted'])\n",
    "df_pred['Observed'] = y_test_eval1sc\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "ax= sns.boxplot(x='Observed',y='Predicted',data=df_pred)\n",
    "ax = sns.stripplot(x=\"Observed\", y=\"Predicted\",data=df_pred)\n",
    "plt.title(\"Predictions after XAI (SHAP, FI)\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# AFTER.5\n",
    "df_pred = pd.DataFrame(y_pred2sc[:, 1], columns = ['Predicted'])\n",
    "df_pred['Observed'] = y_test_eval2sc\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "ax= sns.boxplot(x='Observed',y='Predicted',data=df_pred)\n",
    "ax = sns.stripplot(x=\"Observed\", y=\"Predicted\",data=df_pred)\n",
    "plt.title(\"Predictions after XAI (SHAP, FI*Values)\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# AFTER.6\n",
    "df_pred = pd.DataFrame(y_pred3sc[:, 1], columns = ['Predicted'])\n",
    "df_pred['Observed'] = y_test_eval3sc\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "ax= sns.boxplot(x='Observed',y='Predicted',data=df_pred)\n",
    "ax = sns.stripplot(x=\"Observed\", y=\"Predicted\",data=df_pred)\n",
    "plt.title(\"Predictions after XAI (SHAP, FI*Norm_Values)\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peprec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 24 2022, 14:07:00) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "fd418446a9326e33059c5a832f971392064b382f0c1345c038d317674ac7eb90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
